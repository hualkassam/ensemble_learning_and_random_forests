{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder Hussam Qassim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure \n",
    "MatplotLib plots figures inline and prepare a function to save the figures\n",
    "'''\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ensembles\"\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code creates and trains a voting classifier in Scikit-Learn, composed of three diverse classifiers\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    "                        estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                        voting='hard'\n",
    "                    )\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Let’s look at\teach classifier’s accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "'''\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and\n",
    "predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier.\n",
    "There you have it! The voting classifier slightly outperforms all the individual classifiers. If all \n",
    "classifiers are able to estimate class probabilities (i.e., they have a predict_proba() method), then you can\n",
    "tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual \n",
    "classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives\n",
    "more weight to highly confident votes. All you need to do is replace voting=\"hard\" with voting=\"soft\" and \n",
    "ensure that all classifiers can estimate class probabilities. This is not the case of the SVC class by default,\n",
    "so you need to set its probability hyperparameter to True (this will make the SVC class use cross-validation\n",
    "to estimate class probabilities, slowing down training, and it will add a predict_proba() method). If you \n",
    "modify the preceding code to use soft voting, you will find that the voting classifier achieves over 91% \n",
    "accuracy!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNOTE: The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier\\ncan estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with Decision\\nTrees classifiers.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed.\n",
    "Another approach is to use the same training algorithm for every predictor, but to train them on different\n",
    "random subsets of the training set. When sampling is performed with replacement, this method is called bagging\n",
    "(short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting. In \n",
    "other words, both bagging and pasting allow training instances to be sampled several times across multiple \n",
    "predictors, but only bagging allows training instances to be sampled several times for the same predictor. \n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating\n",
    "the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most\n",
    "frequent prediction, just like a hard voting classifier) for classification, or the average for regression. \n",
    "Each individual predictor has a higher bias than if it were trained on the original training set, but \n",
    "aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias\n",
    "but a lower variance than a single predictor trained on the original training set. Predictors can all be \n",
    "trained in parallel, via different CPU cores or even different servers. Similarly, predictions can be made in\n",
    "parallel. This is one of the reasons why bagging and pasting are such popular methods: they scale very well.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier class (or \n",
    "BaggingRegressor for regression). The following code trains an ensemble of 500 Decision Tree classifiers, 5 \n",
    "each trained on 100 training instances randomly sampled from the training set with replacement (this is an \n",
    "example of bagging, but if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter \n",
    "tells Scikit-Learn the number of CPU cores to use for training and predictions (–1 tells Scikit-Learn to use\n",
    "all available cores)\n",
    "'''\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "                        DecisionTreeClassifier(), n_estimators=500,\n",
    "                        max_samples=100, bootstrap=True, n_jobs=-1\n",
    "                    )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "'''\n",
    "NOTE: The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier\n",
    "can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with Decision\n",
    "Trees classifiers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_clf.oob_score:  0.904\n",
      "Accuracy_score:  0.904\n",
      "bag_clf.oob_decision_function:  [[ 0.38251366  0.61748634]\n",
      " [ 0.32432432  0.67567568]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.0877193   0.9122807 ]\n",
      " [ 0.37777778  0.62222222]\n",
      " [ 0.03932584  0.96067416]\n",
      " [ 0.98734177  0.01265823]\n",
      " [ 0.96808511  0.03191489]\n",
      " [ 0.73493976  0.26506024]\n",
      " [ 0.00591716  0.99408284]\n",
      " [ 0.7970297   0.2029703 ]\n",
      " [ 0.83333333  0.16666667]\n",
      " [ 0.97409326  0.02590674]\n",
      " [ 0.085       0.915     ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.99408284  0.00591716]\n",
      " [ 0.94054054  0.05945946]\n",
      " [ 0.96666667  0.03333333]\n",
      " [ 0.01149425  0.98850575]\n",
      " [ 0.3258427   0.6741573 ]\n",
      " [ 0.90909091  0.09090909]\n",
      " [ 1.          0.        ]\n",
      " [ 0.9673913   0.0326087 ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.59562842  0.40437158]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15104167  0.84895833]\n",
      " [ 0.99453552  0.00546448]\n",
      " [ 0.          1.        ]\n",
      " [ 0.32065217  0.67934783]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.21212121  0.78787879]\n",
      " [ 0.35882353  0.64117647]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.03108808  0.96891192]\n",
      " [ 1.          0.        ]\n",
      " [ 0.00483092  0.99516908]\n",
      " [ 1.          0.        ]\n",
      " [ 0.87777778  0.12222222]\n",
      " [ 0.96858639  0.03141361]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.03351955  0.96648045]\n",
      " [ 0.99473684  0.00526316]\n",
      " [ 0.0047619   0.9952381 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.00564972  0.99435028]\n",
      " [ 0.99494949  0.00505051]\n",
      " [ 0.78735632  0.21264368]\n",
      " [ 0.44705882  0.55294118]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.71428571  0.28571429]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.87209302  0.12790698]\n",
      " [ 1.          0.        ]\n",
      " [ 0.61340206  0.38659794]\n",
      " [ 0.10160428  0.89839572]\n",
      " [ 0.57        0.43      ]\n",
      " [ 0.89528796  0.10471204]\n",
      " [ 0.          1.        ]\n",
      " [ 0.15228426  0.84771574]\n",
      " [ 0.87634409  0.12365591]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.02197802  0.97802198]\n",
      " [ 0.02906977  0.97093023]\n",
      " [ 0.23699422  0.76300578]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.8342246   0.1657754 ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.30810811  0.69189189]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.97409326  0.02590674]\n",
      " [ 0.78531073  0.21468927]\n",
      " [ 0.00529101  0.99470899]\n",
      " [ 1.          0.        ]\n",
      " [ 0.20430108  0.79569892]\n",
      " [ 0.60555556  0.39444444]\n",
      " [ 0.          1.        ]\n",
      " [ 0.02762431  0.97237569]\n",
      " [ 0.48863636  0.51136364]\n",
      " [ 0.99470899  0.00529101]\n",
      " [ 0.04117647  0.95882353]\n",
      " [ 1.          0.        ]\n",
      " [ 0.18954248  0.81045752]\n",
      " [ 0.46073298  0.53926702]\n",
      " [ 1.          0.        ]\n",
      " [ 0.05027933  0.94972067]\n",
      " [ 0.99382716  0.00617284]\n",
      " [ 0.28723404  0.71276596]\n",
      " [ 0.86458333  0.13541667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.76923077  0.23076923]\n",
      " [ 1.          0.        ]\n",
      " [ 0.00529101  0.99470899]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.97765363  0.02234637]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.96236559  0.03763441]\n",
      " [ 0.99481865  0.00518135]\n",
      " [ 0.00606061  0.99393939]\n",
      " [ 0.23976608  0.76023392]\n",
      " [ 0.94652406  0.05347594]\n",
      " [ 0.28648649  0.71351351]\n",
      " [ 0.97938144  0.02061856]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.734375    0.265625  ]\n",
      " [ 0.39759036  0.60240964]\n",
      " [ 0.39226519  0.60773481]\n",
      " [ 0.92349727  0.07650273]\n",
      " [ 0.94886364  0.05113636]\n",
      " [ 0.07446809  0.92553191]\n",
      " [ 0.74456522  0.25543478]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.03960396  0.96039604]\n",
      " [ 0.98924731  0.01075269]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.00546448  0.99453552]\n",
      " [ 0.          1.        ]\n",
      " [ 0.00980392  0.99019608]\n",
      " [ 0.00606061  0.99393939]\n",
      " [ 1.          0.        ]\n",
      " [ 0.99404762  0.00595238]\n",
      " [ 0.94210526  0.05789474]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.99411765  0.00588235]\n",
      " [ 0.          1.        ]\n",
      " [ 0.37209302  0.62790698]\n",
      " [ 0.32663317  0.67336683]\n",
      " [ 0.00510204  0.99489796]\n",
      " [ 0.          1.        ]\n",
      " [ 0.25        0.75      ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.99459459  0.00540541]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.97714286  0.02285714]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.00549451  0.99450549]\n",
      " [ 0.65853659  0.34146341]\n",
      " [ 0.91256831  0.08743169]\n",
      " [ 0.          1.        ]\n",
      " [ 0.99481865  0.00518135]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.07692308  0.92307692]\n",
      " [ 1.          0.        ]\n",
      " [ 0.08675799  0.91324201]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.02424242  0.97575758]\n",
      " [ 1.          0.        ]\n",
      " [ 0.90909091  0.09090909]\n",
      " [ 0.77653631  0.22346369]\n",
      " [ 0.59444444  0.40555556]\n",
      " [ 0.          1.        ]\n",
      " [ 0.18232044  0.81767956]\n",
      " [ 1.          0.        ]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.9787234   0.0212766 ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.52662722  0.47337278]\n",
      " [ 0.83246073  0.16753927]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.99425287  0.00574713]\n",
      " [ 0.01123596  0.98876404]\n",
      " [ 0.          1.        ]\n",
      " [ 0.96685083  0.03314917]\n",
      " [ 0.          1.        ]\n",
      " [ 0.2371134   0.7628866 ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.9673913   0.0326087 ]\n",
      " [ 0.82738095  0.17261905]\n",
      " [ 0.99494949  0.00505051]\n",
      " [ 0.          1.        ]\n",
      " [ 0.09756098  0.90243902]\n",
      " [ 0.9950495   0.0049505 ]\n",
      " [ 0.03448276  0.96551724]\n",
      " [ 0.          1.        ]\n",
      " [ 0.05641026  0.94358974]\n",
      " [ 1.          0.        ]\n",
      " [ 0.80239521  0.19760479]\n",
      " [ 0.          1.        ]\n",
      " [ 0.91578947  0.08421053]\n",
      " [ 0.99418605  0.00581395]\n",
      " [ 0.20689655  0.79310345]\n",
      " [ 0.20487805  0.79512195]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.26237624  0.73762376]\n",
      " [ 0.94270833  0.05729167]\n",
      " [ 0.01036269  0.98963731]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.53763441  0.46236559]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.10994764  0.89005236]\n",
      " [ 0.13068182  0.86931818]\n",
      " [ 0.97969543  0.02030457]\n",
      " [ 0.01086957  0.98913043]\n",
      " [ 1.          0.        ]\n",
      " [ 0.37037037  0.62962963]\n",
      " [ 0.07075472  0.92924528]\n",
      " [ 0.57065217  0.42934783]\n",
      " [ 0.63586957  0.36413043]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.57894737  0.42105263]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.22651934  0.77348066]\n",
      " [ 0.8172043   0.1827957 ]\n",
      " [ 0.07368421  0.92631579]\n",
      " [ 1.          0.        ]\n",
      " [ 0.82162162  0.17837838]\n",
      " [ 0.          1.        ]\n",
      " [ 0.00578035  0.99421965]\n",
      " [ 0.08139535  0.91860465]\n",
      " [ 0.0106383   0.9893617 ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.92352941  0.07647059]\n",
      " [ 0.13333333  0.86666667]\n",
      " [ 0.94329897  0.05670103]\n",
      " [ 0.          1.        ]\n",
      " [ 0.5625      0.4375    ]\n",
      " [ 0.10227273  0.89772727]\n",
      " [ 0.98795181  0.01204819]\n",
      " [ 0.79057592  0.20942408]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.93513514  0.06486486]\n",
      " [ 0.          1.        ]\n",
      " [ 0.00518135  0.99481865]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.24175824  0.75824176]\n",
      " [ 0.99479167  0.00520833]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.88        0.12      ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.75806452  0.24193548]\n",
      " [ 0.91333333  0.08666667]\n",
      " [ 1.          0.        ]\n",
      " [ 0.65841584  0.34158416]\n",
      " [ 0.42076503  0.57923497]\n",
      " [ 0.          1.        ]\n",
      " [ 0.92896175  0.07103825]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.83333333  0.16666667]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.79187817  0.20812183]\n",
      " [ 0.1027027   0.8972973 ]\n",
      " [ 0.49222798  0.50777202]\n",
      " [ 0.22680412  0.77319588]\n",
      " [ 0.          1.        ]\n",
      " [ 0.90049751  0.09950249]\n",
      " [ 0.83333333  0.16666667]\n",
      " [ 0.01724138  0.98275862]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.01630435  0.98369565]\n",
      " [ 0.95833333  0.04166667]\n",
      " [ 0.98360656  0.01639344]\n",
      " [ 1.          0.        ]\n",
      " [ 0.47802198  0.52197802]\n",
      " [ 1.          0.        ]\n",
      " [ 0.00555556  0.99444444]\n",
      " [ 0.99441341  0.00558659]\n",
      " [ 0.01587302  0.98412698]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.98863636  0.01136364]\n",
      " [ 0.          1.        ]\n",
      " [ 0.0505618   0.9494382 ]\n",
      " [ 0.00526316  0.99473684]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.99489796  0.00510204]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.16129032  0.83870968]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.39655172  0.60344828]\n",
      " [ 0.08108108  0.91891892]\n",
      " [ 0.18181818  0.81818182]\n",
      " [ 1.          0.        ]\n",
      " [ 0.98974359  0.01025641]\n",
      " [ 0.21649485  0.78350515]\n",
      " [ 0.97382199  0.02617801]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.94117647  0.05882353]\n",
      " [ 0.35204082  0.64795918]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.99411765  0.00588235]\n",
      " [ 0.          1.        ]\n",
      " [ 0.02747253  0.97252747]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.01052632  0.98947368]\n",
      " [ 0.67032967  0.32967033]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be \n",
    "sampled at all. By default a BaggingClassifier samples m training instances with replacement ( bootstrap=True ),\n",
    "where m is the size of the training set. This means that only about 63% of the training instances are sampled\n",
    "on average for each predictor. The remaining 37% of the training instances that are not sampled are called \n",
    "out-of-bag (oob) instances. Note that they are not the same 37% for all predictors. Since a predictor never \n",
    "sees the oob instances during training, it can be evaluated on these instances, without the need for a separate\n",
    "validation set or cross-validation. You can evaluate the ensemble itself by averaging out the oob evaluations\n",
    "of each predictor. In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an\n",
    "automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation score\n",
    "is available through the oob_score_ variable\n",
    "'''\n",
    "bag_clf = BaggingClassifier(\n",
    "                        DecisionTreeClassifier(), n_estimators=500,\n",
    "                        bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print('bag_clf.oob_score: ', bag_clf.oob_score_)\n",
    "\n",
    "'''\n",
    "According to this oob evaluation, this BaggingClassifier is likely to achieve about 89,86% accuracy on the \n",
    "test set,\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print('Accuracy_score: ',accuracy_score(y_test, y_pred))\n",
    "\n",
    "'''\n",
    "The oob decision function for each training instance is also available through the oob_decision_function_ \n",
    "variable. In this case (since the base estimator has a predict_proba() method) the decision function returns \n",
    "the class probabilities for each training instance. For example, the oob evaluation estimates that the second\n",
    "training instance has a 60.6% probability of belonging to the positive class (and 39.4% of belonging to the\n",
    "positive class)\n",
    "'''\n",
    "print('bag_clf.oob_decision_function: ', bag_clf.oob_decision_function_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe BaggingClassifier class supports sampling the features as well. This is controlled by two hyperparameters:\\nmax_features and bootstrap_features . They work the same way as max_samples and bootstrap, but for feature \\nsampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input \\nfeatures. This is particularly useful when you are dealing with high-dimensional inputs (such as images). \\nSampling both training instances and features is called the Random Patches method. Keeping all training \\ninstances (i.e., bootstrap=False and max_samples=1.0 ) but sampling features (i.e., bootstrap_features=True\\nand/or max_features smaller than 1.0) is called the Random Subspaces method. Sampling features results in even\\nmore predictor diversity, trading a bit more bias for a lower variance.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The BaggingClassifier class supports sampling the features as well. This is controlled by two hyperparameters:\n",
    "max_features and bootstrap_features . They work the same way as max_samples and bootstrap, but for feature \n",
    "sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input \n",
    "features. This is particularly useful when you are dealing with high-dimensional inputs (such as images). \n",
    "Sampling both training instances and features is called the Random Patches method. Keeping all training \n",
    "instances (i.e., bootstrap=False and max_samples=1.0 ) but sampling features (i.e., bootstrap_features=True\n",
    "and/or max_features smaller than 1.0) is called the Random Subspaces method. Sampling features results in even\n",
    "more predictor diversity, trading a bit more bias for a lower variance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "As we have discussed, a Random Forest is an ensemble of Decision Trees, generally trained via the bagging \n",
    "method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of \n",
    "building a BaggingClassifier and passing it a DecisionTreeClassifier , you can instead use the \n",
    "RandomForestClassifier class, which is more convenient and optimized for Decision Trees (similarly, there is \n",
    "a RandomForestRegressor class for regression tasks). The following code trains a Random Forest classifier with\n",
    "500 trees (each limited to maximum 16 nodes), using all available CPU cores\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "'''\n",
    "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to \n",
    "control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble \n",
    "itself. The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for \n",
    "the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias\n",
    "for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly\n",
    "equivalent to the previous RandomForestClassifier\n",
    "'''\n",
    "bag_clf = BaggingClassifier(\n",
    "                        DecisionTreeClassifier(splitter=\"random\",\tmax_leaf_nodes=16),\n",
    "                        n_estimators=500,\tmax_samples=1.0,\tbootstrap=True,\tn_jobs=-1\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra-Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an \\nExtraTreesClassifier. Generally, the only way to know is to try both and compare them using cross-validation\\n(and tuning the hyperparameters using grid search).\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered\n",
    "for splitting. It is possible to make trees even more random by also using random thresholds for each feature\n",
    "rather than searching for the best possible thresholds (like regular Decision Trees do). A forest of such \n",
    "extremely random trees is simply called an Extremely Randomized Trees ensemble (or Extra-Trees for short). Once\n",
    "again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular\n",
    "Random Forests since finding the best possible threshold for each feature at every node is one of the most\n",
    "time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn’s \n",
    "ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the \n",
    "ExtraTreesRegressor class has the same API as the RandomForestRegressor class.\n",
    "'''\n",
    "'''\n",
    "It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an \n",
    "ExtraTreesClassifier. Generally, the only way to know is to try both and compare them using cross-validation\n",
    "(and tuning the hyperparameters using grid search).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.105941014329\n",
      "sepal width (cm) 0.0252674302293\n",
      "petal length (cm) 0.434095488569\n",
      "petal width (cm) 0.434696066872\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Lastly, if you look at a single Decision Tree, important features are likely to appear closer to the root of\n",
    "the tree, while unimportant features will often appear closer to the leaves (or not at all). It is therefore\n",
    "possible to get an estimate of a feature’s importance by computing the average depth at which it appears across\n",
    "all trees in the forest. Scikit-Learn computes this automatically for every feature after training. You can \n",
    "access the result using the feature_importances_ variable. For example, the following code trains a \n",
    "RandomForestClassifier on the iris dataset and outputs each feature’s importance. It seems that the most \n",
    "important features are the petal length (44%) and width (42%), while sepal length and width are rather \n",
    "unimportant in comparison (11% and 2%, respectively)\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name,\tscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak\\nlearners into a strong learner. The general idea of most boosting methods is to train predictors sequentially,\\neach trying to correct its predecessor. There are many boosting methods available, but by far the most popular\\nare AdaBoost (short for Adaptive Boosting) and Gradient Boosting AdaBoost\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak\n",
    "learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially,\n",
    "each trying to correct its predecessor. There are many boosting methods available, but by far the most popular\n",
    "are AdaBoost (short for Adaptive Boosting) and Gradient Boosting AdaBoost\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances\n",
    "that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. \n",
    "This is the technique used by AdaBoost.\n",
    "'''\n",
    "'''\n",
    "Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME (which stands for Stagewise Additive\n",
    "Modeling using a Multiclass Exponential loss function). When there are just two classes, SAMME is equivalent\n",
    "to AdaBoost. Moreover, if the predictors can estimate class probabilities (i.e., if they have a predict_proba() \tmethod),\tScikit-Learn\tcan\tuse\ta\tvariant\tof\tSAMME\tcalled\n",
    "SAMME.R (the R stands for “Real”), which relies on class probabilities rather than predictions and generally\n",
    "performs better.\n",
    "'''\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "                        DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "                        algorithm=\"SAMME.R\", learning_rate=0.5\n",
    "                    )\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies the fraction of \\ntraining instances to be used for training each tree. For example, if subsample=0.25, then each tree is trained\\non 25% of the training instances, selected randomly. As you can probably guess by now, this trades a higher \\nbias for a lower variance. It also speeds up training considerably. This technique is called Stochastic \\nGradient Boosting\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Another very popular Boosting algorithm is Gradient Boosting. Just like AdaBoost, Gradient Boosting works by\n",
    "sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of \n",
    "tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new \n",
    "predictor to the residual errors made by the previous predictor. Let’s go through a simple regression example\n",
    "using Decision Trees as the base predictors (of course Gradient Boosting also works great with regression \n",
    "tasks). This is called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s fit \n",
    "a DecisionTreeRegressor to the training set (for example, a noisy quadratic training set)\n",
    "'''\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "# Now train a second DecisionTreeRegressor on the residual errors made by the first predictor\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "# Then we train a third regressor on the residual errors made by the second predictor\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)\n",
    "'''\n",
    "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding \n",
    "up the predictions of all the trees\n",
    "'''\n",
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "'''\n",
    "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRegressor class. Much like the\n",
    "RandomForestRegressor class, it has hyperparameters to control the growth of Decision Trees (e.g., max_depth ,\n",
    "min_samples_leaf , and so on), as well as hyperparameters to control the ensemble training, such as the number\n",
    "of trees ( n_estimators ). The following code creates the same ensemble as the previous one\n",
    "'''\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)\n",
    "\n",
    "'''\n",
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such as\n",
    "0.1 , you will need more trees in the ensemble to fit the training set, but the predictions will usually\n",
    "generalize better. This is a regularization technique called shrinkage.\n",
    "In order to find the optimal number of trees, you can use early stopping. A simple way to implement this is to \n",
    "use the staged_predict() method: it returns an iterator over the predictions made by the ensemble at each \n",
    "stage of training (with one tree, two trees, etc.). The following code trains a GBRT ensemble with 120 trees, \n",
    "then measures the validation error at each stage of training to find the optimal number of trees, and finally \n",
    "trains another GBRT ensemble using the optimal number of trees\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred)          \n",
    "            for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "'''\n",
    "It is also possible to implement early stopping by actually stopping training early (instead of training a\n",
    "large number of trees first and then looking back to find the optimal number). You can do so by setting \n",
    "warm_start=True , which makes Scikit-Learn keep existing trees when the fit() method is called, allowing \n",
    "incremental training. The following code stops training when the validation error does not improve for five \n",
    "iterations in a row\n",
    "'''\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping\n",
    "            \n",
    "'''\n",
    "The GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies the fraction of \n",
    "training instances to be used for training each tree. For example, if subsample=0.25, then each tree is trained\n",
    "on 25% of the training instances, selected randomly. As you can probably guess by now, this trades a higher \n",
    "bias for a lower variance. It also speeds up training considerably. This technique is called Stochastic \n",
    "Gradient Boosting\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The last Ensemble method we will discuss in this chapter is called stacking (short for stacked generalization).\n",
    "It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the \n",
    "predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? Each of\n",
    "the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \n",
    "(called a blender, or a meta learner) takes these predictions as inputs and makes the final prediction\n",
    "'''\n",
    "'''\n",
    "To train the blender, a common approach is to use a hold-out set. Let’s see how it works. First, the training \n",
    "set is split in two subsets. The first subset is used to train the predictors in the first layer. Next, the \n",
    "first layer predictors are used to make predictions on the second (held-out) set. This ensures that the \n",
    "predictions are “clean,” since the predictors never saw these instances during training. Now for each instance\n",
    "in the hold-out set there are three predicted values. We can create a new training set using these predicted\n",
    "values as input features (which makes this new training set three-dimensional), and keeping the target values.\n",
    "The blender is trained on this new training set, so it learns to predict the target value given the first \n",
    "layer’s predictions. It is actually possible to train several different blenders this way (e.g., one using \n",
    "Linear Regression, another using Random Forest Regression, and so on): we get a whole layer of blenders. The\n",
    "trick is to split the training set into three subsets: the first one is used to train the first layer, the \n",
    "second one is used to create the training set used to train the second layer (using predictions made by the\n",
    "predictors of the first layer), and the third one is used to create the training set to train the third layer\n",
    "(using predictions made by the predictors of the second layer). Once this is done, we can make a prediction for\n",
    "a new instance by going through each layer sequentially. Unfortunately, Scikit-Learn does not support stacking\n",
    "directly, but it is not too hard to roll out your own implementation (see the following exercises). \n",
    "Alternatively, you can use an open source implementation such as brew\n",
    "(available at https://github.com/viisar/brew).\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
